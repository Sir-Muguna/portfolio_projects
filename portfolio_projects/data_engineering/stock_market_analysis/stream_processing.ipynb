{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection established.\n",
      "Table created or already exists.\n",
      "Error initializing Kafka consumer: NoBrokersAvailable\n"
     ]
    },
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Initialize Kafka consumer\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     consumer \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaConsumer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkafka_topic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkafka_bootstrap_servers\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_deserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKafka consumer initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/kafka/consumer/group.py:364\u001b[0m, in \u001b[0;36mKafkaConsumer.__init__\u001b[0;34m(self, *topics, **configs)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, str_version\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    361\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse api_version=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m [tuple] -- \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as str is deprecated\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    362\u001b[0m                 \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m]), str_version)\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkafka_client\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Get auto-discovered version from client if necessary\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/kafka/client_async.py:234\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     check_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version_auto_timeout_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_upon_socket_err_during_wakeup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise_upon_socket_err_during_wakeup\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/kafka/client_async.py:902\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoBrokersAvailable()\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_connect(try_node)\n\u001b[1;32m    904\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conns[try_node]\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "from kafka import KafkaConsumer\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "kafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\n",
    "kafka_topic = os.getenv(\"KAFKA_TOPIC\")\n",
    "pg_host = os.getenv(\"PG_HOST\")\n",
    "pg_port = os.getenv(\"PG_PORT\")\n",
    "pg_db = os.getenv(\"PG_DB\")\n",
    "pg_user = os.getenv(\"PG_USER\")\n",
    "pg_password = os.getenv(\"PG_PASSWORD\")\n",
    "pg_table = os.getenv(\"PG_TABLE\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToPostgres\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set up PostgreSQL connection\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=pg_db, \n",
    "        user=pg_user, \n",
    "        password=pg_password, \n",
    "        host=pg_host, \n",
    "        port=pg_port\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"PostgreSQL connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create the stock_data table if it doesn't exist\n",
    "try:\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {pg_table} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            date TIMESTAMP,\n",
    "            open NUMERIC,\n",
    "            high NUMERIC,\n",
    "            low NUMERIC,\n",
    "            close NUMERIC,\n",
    "            volume BIGINT\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"Table created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "    conn.rollback()\n",
    "    raise\n",
    "\n",
    "# Initialize Kafka consumer\n",
    "try:\n",
    "    consumer = KafkaConsumer(\n",
    "        kafka_topic,\n",
    "        bootstrap_servers=[kafka_bootstrap_servers],\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    "    )\n",
    "    print(\"Kafka consumer initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Kafka consumer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Process messages from Kafka\n",
    "try:\n",
    "    for message in consumer:\n",
    "        data = message.value\n",
    "        print(f\"Received message: {data}\")\n",
    "\n",
    "        # Convert the data to a PySpark DataFrame\n",
    "        try:\n",
    "            df = spark.createDataFrame([data])\n",
    "\n",
    "            # Apply transformations\n",
    "            df_transformed = df.select(\n",
    "                col('symbol'),\n",
    "                col('date').cast('timestamp'),\n",
    "                col('open').cast('decimal(10,2)'),\n",
    "                col('high').cast('decimal(10,2)'),\n",
    "                col('low').cast('decimal(10,2)'),\n",
    "                col('close').cast('decimal(10,2)'),\n",
    "                col('volume').cast('long')\n",
    "            )\n",
    "\n",
    "            # Convert DataFrame to Pandas for PostgreSQL insertion\n",
    "            df_pandas = df_transformed.toPandas()\n",
    "\n",
    "            # Insert data into PostgreSQL table\n",
    "            try:\n",
    "                for _, row in df_pandas.iterrows():\n",
    "                    cur.execute(f\"\"\"\n",
    "                        INSERT INTO {pg_table} (symbol, date, open, high, low, close, volume)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\", (\n",
    "                        row['symbol'],\n",
    "                        row['date'],\n",
    "                        row['open'],\n",
    "                        row['high'],\n",
    "                        row['low'],\n",
    "                        row['close'],\n",
    "                        row['volume']\n",
    "                    ))\n",
    "                conn.commit()\n",
    "                print(\"Data inserted into PostgreSQL.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting data into PostgreSQL: {e}\")\n",
    "                conn.rollback()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data: {e}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down Kafka consumer...\")\n",
    "finally:\n",
    "    # Clean up and close connections\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    spark.stop()\n",
    "    print(\"Connections closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
